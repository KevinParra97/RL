{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import MeanSquaredError\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def q_network():\n",
    "    \n",
    "    '''\n",
    "    Creamos nuestro agente, en este caso es tan sencillo como una NN que toma como entrada el estado\n",
    "    y devuelve la acción a realizar. Está compuesta por una capa de entrada, una capa oculta fully connected de 128 nodos\n",
    "    y una capa de salida de 2 nodos (correspondiente a las 2 posibles acciones del entorno cartpole)\n",
    "    '''\n",
    "    # Inicializamos la red neuronal\n",
    "    net = Sequential(name='Fully-Connected-Network')\n",
    "\n",
    "    # Añadimos la capa oculta y le indicamos que el input \n",
    "    # debe corresponder al espacio de observación\n",
    "    # Inicializamos los pesos de forma aleatoria uniformemente distribuida\n",
    "    # Indicamos una función de activación para esa capa oculta de tipo ReLU\n",
    "\n",
    "    net.add(Dense(\n",
    "        128, \n",
    "        input_shape=(ENV.observation_space.n,), \n",
    "        name='Hidden',\n",
    "        kernel_initializer='random_uniform',\n",
    "        bias_initializer='zeros',\n",
    "        activation='relu'\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Añadimos una última capa de salida con 2 nodos y función de activación relu.\n",
    "    # Esta función de activación nos devuelve la Q de cada acción en el estado introducido en el input\n",
    "    \n",
    "    net.add(Dense(\n",
    "        ENV.action_space.n, \n",
    "        name='Output',\n",
    "        kernel_initializer='random_uniform',\n",
    "        bias_initializer='zeros',\n",
    "        activation='relu')\n",
    "       )\n",
    "\n",
    "    # Obtenemos información de esta red \n",
    "    net.summary()\n",
    "\n",
    "    # La compilamos para su posterior entrenamiento con el optimizador Adam y\n",
    "    # la función de pérdida de tipo MeanSquaredError() típica de ejercicios de regresión (como este)\n",
    "    # Y utilizada por DeepMind en su paper sobre DQN\n",
    "    \n",
    "    net.compile(\n",
    "        optimizer=Adam(), \n",
    "        loss=MeanSquaredError(), \n",
    "        metrics=['acc'])\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "from random import random\n",
    "\n",
    "def get_action_and_value_epsilon_greedy(q_values):\n",
    "    \n",
    "    '''\n",
    "    Ejecuta una política epsilon-greedy para seleccionar la acción a realizar\n",
    "    @ q_values: list --> Lista con los valores de Q predichos por la red neuronal \n",
    "    '''\n",
    "    \n",
    "    # Actuar random\n",
    "    if random() < EPSILON:\n",
    "        \n",
    "        action, action_q_value = get_random_action_and_value(q_values)\n",
    "            \n",
    "    # Actuar Greedy:\n",
    "    else:\n",
    "        \n",
    "        action, action_q_value = get_best_action_and_value(q_values)\n",
    "    \n",
    "    \n",
    "    return action, action_q_value\n",
    "    \n",
    "def get_best_action_and_value(q_values):\n",
    "    \n",
    "    '''\n",
    "    Devuelve la acción con el Q más alto y su valor (Q)\n",
    "    @ q_values: list --> Lista con los valores de Q predichos por la red neuronal \n",
    "    '''\n",
    "    \n",
    "    best_action = argmax(q_values) # 0 ó 1\n",
    "    best_q_value = q_values[best_action] # Q más alto\n",
    "    \n",
    "    return best_action, best_q_value\n",
    "\n",
    "\n",
    "def get_random_action_and_value(q_values):\n",
    "    \n",
    "    '''\n",
    "    Devuelve una acción aleatoria y su valor (Q)\n",
    "    @ q_values: list --> Lista con los valores de Q predichos por la red neuronal \n",
    "    '''\n",
    "    \n",
    "    random_action = ENV.action_space.sample()\n",
    "    q_value_random_action = q_values[random_action]\n",
    "    \n",
    "    return random_action, q_value_random_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from numpy import expand_dims, argmax\n",
    "import gym\n",
    "\n",
    "def reward_from_greedy_episode(network, render=False):\n",
    "    \n",
    "    '''\n",
    "    Ejecutamos la red en el entorno y obtenemos la recompensa final\n",
    "    acumulada de ese entorno.\n",
    "    '''\n",
    "    \n",
    "    # Iniciamos el entorno\n",
    "    environment = gym.make('FrozenLake-v0')\n",
    "    # Obtenemos la primera observación\n",
    "    obs = environment.reset()\n",
    "    # Indicamos que el episodio NO ha acabado\n",
    "    done = False\n",
    "    # Iniciamos el contador de recompensas a 0\n",
    "    total_rew = 0\n",
    "    \n",
    "    # Mientras el episodio no haya acabado\n",
    "    while not done:\n",
    "        \n",
    "        # Transoformamos una observación a un BATCH de una única observación\n",
    "        obs_vector = expand_dims(obs, axis=0)\n",
    "        # Obtenemos el Q de las acciones para el estado actual\n",
    "        q_actions = network.predict(obs_vector)[0]\n",
    "        # Tomamos la acción con mayor Q --> política greedy\n",
    "        action = argmax(q_actions)\n",
    "        # ejecutamos la acción en el entorno y obtenemos el estado alcanzado, \n",
    "        # la recompensa y si el episodio ha terminado\n",
    "        obs, rew, done, _ = environment.step(action)\n",
    "        # acumulamos la recompensa obtenida a la recompensa total\n",
    "        total_rew += rew\n",
    "        # renderizamos el frame if True\n",
    "        if render: environment.render() \n",
    "    \n",
    "    # Al terminar cerramos el entorno\n",
    "    environment.close()\n",
    "    \n",
    "    # Devolvemos la recompensa total\n",
    "    return total_rew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "from random import sample\n",
    "from numpy import array\n",
    "\n",
    "class memory_replay:\n",
    "    \n",
    "    '''\n",
    "    Objeto que acumula los siguientes datso en cada paso sobre el entorno:\n",
    "    - El estado actual en un momento t\n",
    "    - La acción realizada en ese estado en el momento t\n",
    "    - El estado alcanzado en el momento t+1\n",
    "    - La recompensa obtenida por pasar del estado actual al estado alcanzado\n",
    "    - Un parámetro True/False que determina si el estado alcanzado es un estado terminal (si el episodio ha terminado)\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, length):\n",
    "        \n",
    "        '''\n",
    "        Creamos una lista especial (deque) en la que almacenar los parámetros mencionados antes\n",
    "        Para mejor organización creamos una tupla con nombres para acceder más fácilmente a cada parámetro\n",
    "        '''\n",
    "        \n",
    "        self.memory_buffer = deque([], maxlen=length)\n",
    "        self.mem_vector = namedtuple(\n",
    "            'mem_vector',\n",
    "            field_names = ['actual_state', 'action', 'next_state', 'reward', 'done']\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def memorize(self, actual_state, action, next_state, reward, done):\n",
    "        \n",
    "        '''\n",
    "        Almacena cada parámetro en una tupla con nombres y esa tupla en el deque\n",
    "        '''\n",
    "        \n",
    "        vector = self.mem_vector(\n",
    "            actual_state = actual_state,\n",
    "            action = action, \n",
    "            next_state = next_state, \n",
    "            reward = reward, \n",
    "            done = done\n",
    "        )\n",
    "        \n",
    "        self.memory_buffer.append(vector)\n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        '''\n",
    "        formatea el buffer de memoria\n",
    "        '''\n",
    "        \n",
    "        self.memory_buffer.clear()\n",
    "      \n",
    "    def buffer_size(self):\n",
    "        \n",
    "        '''\n",
    "        Devuelve el número de instancias guardadas hasta el momento\n",
    "        '''\n",
    "        \n",
    "        return len(self.memory_buffer)\n",
    "        \n",
    "    def get_batch(self, n):\n",
    "        \n",
    "        '''\n",
    "        Devuelve, de entre todas las instancias almacenadas, un conjunto aleatorio de:\n",
    "        - Array con los estados actuales\n",
    "        - Array con las acciones realizadas en esos estados\n",
    "        - Array con los estados alcanzados por realizar esas acciones en esos estados actuales\n",
    "        - Array con las recompensas obtenidas por alcanzar dichos estados \n",
    "        - Array con valores True/False que indican si el episodio se ha terminado en el estado alcanzado\n",
    "        @ n: int --> Número de instancias que queremos devolver\n",
    "        '''\n",
    "        \n",
    "        # Sanity Check: evitamos pedir más instancias de las que tenemos almacenadas\n",
    "        \n",
    "        if n > len(self.memory_buffer):\n",
    "            \n",
    "            raise Exception(\"No hay suficientes instancias: se solicitaron {} y hay {}\".format(n , len(self.memory_buffer)) )\n",
    "        \n",
    "        # Creamos otra namedtuple temporal para organizar los arrays mencionados dentro de un único objeto\n",
    "        \n",
    "        mem_batch = namedtuple(\n",
    "            'mem_batch',\n",
    "            field_names = ['actual_states', 'actions', 'next_states', 'rewards', 'dones']\n",
    "        )\n",
    "        \n",
    "        # Tomamos de la memoria un número aleatorio n de instancias\n",
    "        \n",
    "        instances = sample(self.memory_buffer, n)\n",
    "        \n",
    "        # De esas instancias extraemos: sus estados actuales, acciones, estados alcanzados, recompensas \n",
    "        # y si el episodio terminó o no en el estado alcanzado\n",
    "        \n",
    "        actual_state_sample = array([instance.actual_state for instance in instances])  # Array con los estados actuales\n",
    "        action_sample = array([instance.action for instance in instances]) # Array con las acciones realizadas en esos estados\n",
    "        next_state_sample = array([instance.next_state for instance in instances]) # Array con los estados alcanzados\n",
    "        reward_sample = array([instance.reward for instance in instances]) # Array con las recompensas obtenidas\n",
    "        done_sample = array([instance.done for instance in instances]) # Array True/False si el episodio ha terminado\n",
    "        \n",
    "        # Empaquetamos los arrays en un único objeto para mejorar la transcripción del código\n",
    "        \n",
    "        batch = mem_batch(\n",
    "            actual_states = actual_state_sample, \n",
    "            actions = action_sample, \n",
    "            next_states = next_state_sample, \n",
    "            rewards = reward_sample, \n",
    "            dones = done_sample\n",
    "        )\n",
    "        \n",
    "        # Devolvemos el Batch\n",
    "        \n",
    "        return batch\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Iniciando Simulación\n",
      "[*] Número de épocas: 20\n",
      "[*] Número de episodios por época: 50\n",
      "Model: \"Fully-Connected-Network\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Hidden (Dense)               (None, 128)               2176      \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 2,692\n",
      "Trainable params: 2,692\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"Fully-Connected-Network\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Hidden (Dense)               (None, 128)               2176      \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 2,692\n",
      "Trainable params: 2,692\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "We are at epoc: 1 Epsilon 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected Hidden_input to have shape (16,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-281cc787a235>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;31m# Probamos nuestra red en el entorno durante un episodio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mepisode_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_from_greedy_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_network_local\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# min: 0, max: 200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;31m# Incorporamos la recompensa de prueba del episodio al historial de la época\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-990c28afc9f7>\u001b[0m in \u001b[0;36mreward_from_greedy_episode\u001b[0;34m(network, render)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mobs_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Obtenemos el Q de las acciones para el estado actual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mq_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Tomamos la acción con mayor Q --> política greedy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \u001b[0;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1441\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1442\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected Hidden_input to have shape (16,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "from numpy import expand_dims, copy, mean\n",
    "from keras.losses import mean_squared_error\n",
    "import gym\n",
    "\n",
    "'''\n",
    "PASO 0: Creamos el entorno, determinamos el número de épocas y el número de episodios a realizar en cada época.\n",
    "Creamos Epsilon y su factor de descuento. Determinamos el tamaño del Batch de entrenamiento. Y finalmente\n",
    "determinamos Gamma.\n",
    "\n",
    "'''\n",
    "\n",
    "# Creamos el entorno\n",
    "ENV = gym.make('FrozenLake-v0')\n",
    "\n",
    "# Definimos el número de épocas y episodios\n",
    "EPOCS = 20\n",
    "EPISODES = 50\n",
    "\n",
    "# Definimos Epsilon (que guía la política epsilon-greedy)\n",
    "# Y como se irá reduciendo con el tiempo\n",
    "EPSILON = 1\n",
    "EPSILON_DECAY = 0.05\n",
    "EPSILON_MIN = 0.01\n",
    "\n",
    "# Definimos el tamaño del Batch que usaremos en la fase de entrenamiento\n",
    "# y el número de épocas que se entrenará la red sobre ese Batch\n",
    "TRAINING_BATCH_SIZE = 128\n",
    "TRAINING_EPOCH = 32\n",
    "# TRAINING_VALIDATION_SPLIT = 0.1\n",
    "\n",
    "# Factor Gamma de Descuento (en clases anteriores vimos que es una forma de \n",
    "# reducir las expectativas de recompensa que esperamos obtener a largo plazo)\n",
    "GAMMA = 0.9\n",
    "\n",
    "print('[*] Iniciando Simulación')\n",
    "print('[*] Número de épocas:', EPOCS)\n",
    "print('[*] Número de episodios por época:', EPISODES)\n",
    "\n",
    "# Creamos las dos redes \"gemelas\" que definen las estimaciones de Q en s (Q_network_local)\n",
    "# y las estimaciones de Q en s' (Q_network_target)\n",
    "q_network_local = q_network()\n",
    "q_network_target = q_network()\n",
    "\n",
    "# Inicializamos la memoria \n",
    "memory = memory_replay(20000)\n",
    "\n",
    "# Una lista vacía donde almacenar todo el progreso de la red en el entorno de pruebas\n",
    "training_rewards = []\n",
    "\n",
    "'''\n",
    "PASO 1: INTERACCIÓN\n",
    "Por cada época se ejecuta, primero, un número determinado de episodios, tras cada episodio se determina si hay\n",
    "suficientes instancias para entrenar la red \"local\", si es así iniciamos el entrenamiento de esa red. Al terminar\n",
    "el episodio actualizamos la red \"objetivo\".\n",
    "'''\n",
    "\n",
    "for epoc in range(EPOCS):\n",
    "    \n",
    "    print(\"We are at epoc:\", epoc+1, \"Epsilon\", EPSILON)\n",
    "    \n",
    "    # Reducción de epsilon tras cada época\n",
    "    if EPSILON > EPSILON_MIN:\n",
    "        EPSILON -= (EPSILON*EPSILON_DECAY)\n",
    "    \n",
    "    # Reiniciamos la memoria a 0\n",
    "    memory.reset()\n",
    "\n",
    "    # Una lista vacía donde almacenar el progreso de la red en el entorno de pruebas tras cada época\n",
    "    epoc_rewards = []\n",
    "\n",
    "    # Por cada episodio dentro de la época actual\n",
    "    for episode in range(EPISODES):\n",
    "        \n",
    "        # Determinamos que el episodio NO ha acabado\n",
    "        episode_is_done = False\n",
    "        \n",
    "        # Reseteamos el entorno y obtenemos nuestra primera observación (observación == estado)\n",
    "        actual_state = ENV.reset()\n",
    "        \n",
    "        # Mientras el episodio no haya terminado\n",
    "        while not episode_is_done:\n",
    "            \n",
    "            # Ampliamos la dimensión de las observaciones para que puedan ser procesadas por la red.\n",
    "            # Recordemos que las redes en Keras admiten CONJUNTOS de instancias y NO Instancias sueltas.\n",
    "            # Por Ejemplo: la observación actual (== estado actual) podría ser en este entorno:\n",
    "            # (-0.00512, 1.25848, 0.222658, 28.54845) --> variables continuas (nos \"da igual\" qué representa cada valor)\n",
    "            # la observación actual tiene un \"shape\" de (4,) pero keras solo acepta BATCHES (conjuntos de observaciones)\n",
    "            # por lo que debemos transformar su \"shape\" a (1, 4) lo cual se traduce como un BATCH de UNA ÚNICA INSTANCIA (4,)\n",
    "            # es decir, un conjunto de instancias formado por UNA ÚNICA INSTANCIA.\n",
    "            actual_state = to_categorical(\n",
    "                actual_state, \n",
    "                num_classes=ENV.observation_space.n\n",
    "            )\n",
    "            actual_state_batch = expand_dims(actual_state, axis=0)\n",
    "            \n",
    "            # Obtenemos el valor de Q de las acciones en el estado acual gracias a neustra q_network_local\n",
    "            # De nuevo Keras nos devuelve un CONJUNTO de PREDICCIONES, como sólo hemos \"pasado\" una observación\n",
    "            # obtenemos tan sólo una predicción, de ahí que nos quedemos con la primera (y única) '[0]'\n",
    "      \n",
    "            q_values_actual_state = q_network_local.predict(actual_state_batch)[0]\n",
    "            \n",
    "            # Obtenemos la acción a realizar siguiendo una política Epsilon-Greedy\n",
    "            \n",
    "            selected_action, q_selected_action = get_action_and_value_epsilon_greedy(q_values_actual_state)\n",
    "            \n",
    "            # Una vez seleccionada la acción a realizar la aplicamos sobre el entorno y obtenemos\n",
    "            # el estado alcanzado, la recompensa de transición, y si el episodio ha terminado\n",
    "            \n",
    "            next_state, reward, episode_is_done, _ = ENV.step(selected_action)\n",
    "            \n",
    "            # Guardamos todos los valores en nuetra memoria para usarlos en el entrenamiento\n",
    "            \n",
    "            memory.memorize(actual_state, selected_action, next_state, reward, episode_is_done)\n",
    "            \n",
    "            # El estado alcanzado es ahora el estado actual, y repetimos el loop hasta que termine el episodio.\n",
    "            \n",
    "            actual_state = next_state\n",
    "            \n",
    "        \n",
    "        '''\n",
    "        PASO 1: ENTRENAMIENTO\n",
    "        Tras terminar la fase de interacción llega la fase de entrenamiento donde entrenamos UNA de las dos redes.\n",
    "        La red a entrenar es SIEMPRE la local, la red OBJETIVO no se actualiza (aún) ya que de ella obtenemos las \n",
    "        expectativas de recompensa a futuro Q(s', a(max)). Si actualizáramos las dos a la vez nuestro \"objetivo\"\n",
    "        cambiaría con cada entrenamiento y eso desestabilizaría dicho entrenamiento. Este último punto es algo más\n",
    "        difícil de entender, pero es la clave del algoritmo DQN, por lo que os aconsejo que intentéis interiorizarlo\n",
    "        para comprender la clase entera.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        # Entrenamos sólo si hay instancias suficientes\n",
    "        if memory.buffer_size() >= TRAINING_BATCH_SIZE:\n",
    "            \n",
    "            # Obtenemos un batch aleatorio de instancias recolectadas durante la fase de interacción\n",
    "            batch = memory.get_batch(TRAINING_BATCH_SIZE)\n",
    "\n",
    "            actual_states = batch.actual_states\n",
    "            actions = batch.actions\n",
    "            next_states = batch.next_states\n",
    "            rewards = batch.rewards\n",
    "            dones = batch.dones\n",
    "            \n",
    "            # Obtenemos los valores de Q para el vector de estados actuales. Por cada estado actual obtenemos\n",
    "            # una lista con dos valores (para el entorno CARTPOLE), esos dos valores determinan el Q de cada una\n",
    "            # de las 2 acciones posibles dentro de cada uno de esos estados.\n",
    "            q_estimation = q_network_local.predict(actual_states) \n",
    "            \n",
    "            # Obtenemos los valores de Q para el vector de estados alcanzados. Por cada estado alcanzado obtenemos\n",
    "            # una lista con dos valores (para el entorno CARTPOLE), esos dos valores determinan el Q de cada una\n",
    "            # de las 2 acciones posibles dentro de cada uno de esos estados.\n",
    "            q_target = q_network_target.predict(next_states)\n",
    "            \n",
    "            # De esos 2 Q nos quedamos con el más alto siguiendo las directrices del algoritmo Q-learning\n",
    "            q_target = q_target.max(1)\n",
    "            \n",
    "            # Si el estado alcanzado del que obtenemos Q es un estado FINAL entonces su Q = 0 !!! \n",
    "            # Recordamos: Q es el valor de una acción, determina la cantidad de recompensa que esperamos \n",
    "            # obtener a largo plazo desde un estado cualquiera realizando dicha acción. Al ser un estado TERMINAL\n",
    "            # la expectativa de recompensas DEBE SER 0, ya que el episodio HA TERMINADO, y no podemos obtener\n",
    "            # nuevas recompensas.\n",
    "            q_target[dones] = 0 # donde done == True, q_target = 0\n",
    "            \n",
    "            # Finalmente aplicamos BELLMAN para determinar el objetivo.\n",
    "            # Recordamos: lo llamamos objetivo porque idealmente queremos que nuestra red LOCAL (ESTIMACIÓN)\n",
    "            # nos de como resultado dicho OBJETIVO. Por esto precisamente NO actualizamos la red OBJETIVO tras \n",
    "            # cada episodio, porque necesitamos que el OBJETIVO sea EL MISMO durante la fase de entrenamiento\n",
    "            # de la red LOCAL, para no \"liar\" a la red local.\n",
    "            \n",
    "            target = rewards + GAMMA * q_target # lista de valoroes objetivo para cada instancia\n",
    "            \n",
    "            # El siguiente paso idealmente sería algo así:\n",
    "            # loss = MEANSQUAREDERROR(target - q_estimation(actions))\n",
    "            # q_network_local.backpropagation(loss)\n",
    "            # Pero Keras NO permite esto de una forma relativamente sencilla, por lo que, para no hacer más complejo\n",
    "            # el código usaremos un \"SMART-TRICK\" jugando con la función de pérdida \"loss\":\n",
    "            \n",
    "            # Vamos a tomar el vector de estimaciones de Q para los estados actuales, los cual nos dan el valor de Q\n",
    "            # para cada una de las dos acciones posibles para cada uno de esos estados, y vamos a cambiar el valor de Q \n",
    "            # de la acción que realizamos en su momento por el valor del objetivo.\n",
    "            \n",
    "            Y = copy(q_estimation) # copia de los resultados de q_network_local para los estados actuales del batch\n",
    "            \n",
    "            for index, action in enumerate(actions): # Por cada acción dentro del conjunto de acciones del batch\n",
    "                \n",
    "                Y[index, action] = target[index] # Cambiamos el valor Y para la acción realizada para que sea = target\n",
    "            \n",
    "            # Al introducir de nuevo durante el \n",
    "            # entrenamiento el estado actual como INPUT obtendremos los valores de Q para el estado actual, dichos valores\n",
    "            # son idénticos a los que teníamos en q_estimation, idénticos todos, salvo el de la acción ejecutada que ahora\n",
    "            # es igual al target del estado alcanzado. Por tanto, cuando se aplique la función de pérdida \"MSE\"\n",
    "            # que recordemos que es (target - q_estimation(actions))^2 --> (target - q_estimation) será 0 para todas las \n",
    "            # acciones menos para la acción realizada por lo que los pesos se actualizarán UNICAMENTE para la acción realizada.\n",
    "            \n",
    "            q_network_local.fit(actual_states,\n",
    "                         Y,\n",
    "                         batch_size=TRAINING_BATCH_SIZE,\n",
    "                         epochs=TRAINING_EPOCH,\n",
    "                         verbose=0)\n",
    "            \n",
    "            # Al final, tras el entrenamiento, copiamos los pesos de la q_network_local a la q_network_target\n",
    "        \n",
    "            q_network_target.set_weights(q_network_local.get_weights())\n",
    "    \n",
    "        '''\n",
    "        OPCIONAL: controlar el rendimiento de nuestra red (nuestro agente) en un entorno de pruebas\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        # Probamos nuestra red en el entorno durante un episodio\n",
    "        episode_reward = reward_from_greedy_episode(q_network_local, render=False) # min: 0, max: 200\n",
    "        \n",
    "        # Incorporamos la recompensa de prueba del episodio al historial de la época\n",
    "        epoc_rewards.append(episode_reward)\n",
    "        \n",
    "    # Incorporamos las recompensas de prueba de cada epoca al historial total\n",
    "    training_rewards.extend(epoc_rewards)\n",
    "    \n",
    "    print(\"The best reward at EPOC {} is {} with mean {}\".format(epoc+1, max(epoc_rewards), mean(epoc_rewards)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13f34e940>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(training_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
